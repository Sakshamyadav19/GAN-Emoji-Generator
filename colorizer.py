# -*- coding: utf-8 -*-
"""Colorizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEVWzfHxKrXw9c8X9yTixKGOfmIJOgEt
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d subinium/emojiimage-dataset
!unzip emojiimage-dataset.zip -d emoji_dataset

import pandas as pd

df = pd.read_csv("emoji_dataset/full_emoji.csv")
print("Columns:", df.columns)
df.head(10)

import os
from PIL import Image
import pandas as pd

# Load CSV
df = pd.read_csv("emoji_dataset/full_emoji.csv")

# Platforms (folder names under emoji_dataset/image/)
platforms = ['Apple', 'Google', 'Facebook', 'Windows', 'Twitter',
             'JoyPixels', 'Samsung', 'Gmail', 'SoftBank', 'DoCoMo', 'KDDI']

# Output folder
out_dir = "emoji_faces"
os.makedirs(out_dir, exist_ok=True)

def process_image(in_path, out_path):
    try:
        img = Image.open(in_path).convert("RGB")
        img = img.resize((32, 32), Image.BILINEAR)
        img.save(out_path)
    except Exception as e:
        print(f"Error: {in_path}", e)

# Main loop
for idx, row in df.iterrows():
    emoji_id = row['#']  # This is the image filename

    for platform in platforms:
        in_path = f"emoji_dataset/image/{platform}/{emoji_id}.png"
        out_path = os.path.join(out_dir, f"emoji__{platform}__{emoji_id}.png")

        if os.path.exists(in_path):
            process_image(in_path, out_path)

from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

from torch.utils.data import Dataset
from PIL import Image
import os
import glob

class EmojiDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.transform = transform
        # Collect all .png images recursively from emoji_faces/
        self.emoji_list = glob.glob(os.path.join(image_dir, '**', '*.png'), recursive=True)

    def __getitem__(self, idx):
        path = self.emoji_list[idx]
        image = Image.open(path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image

    def __len__(self):
        return len(self.emoji_list)

dataset = EmojiDataset("emoji_faces", transform=transform)
print("Total images:", len(dataset))

import torch

dataLoader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)

import matplotlib.pyplot as plt
import torchvision.utils as vutils

# Get 1 batch
images = next(iter(dataLoader))  # shape: [128, 3, 64, 64]

# Un-normalize: from [-1,1] back to [0,1]
images = images * 0.5 + 0.5

# Plot a grid of 32 images
plt.figure(figsize=(10,10))
plt.axis("off")
plt.title("Sample Training Images")
plt.imshow(vutils.make_grid(images[:32], nrow=8).permute(1, 2, 0))
plt.show()

import torch.nn as nn

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(64, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            # TODO: First block (input is 3x64x64 image)
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.BatchNorm2d(64),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Dropout(0.3),
            nn.BatchNorm2d(128),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.BatchNorm2d(256),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.BatchNorm2d(512),
            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

import torch.nn as nn

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

import torch

# Automatically use GPU if available, else fall back to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

gen = Generator().to(device)
disc = Discriminator().to(device)

gen.apply(weights_init)
disc.apply(weights_init)

import torch.nn as nn
import torch.optim as optim

criterion = nn.BCELoss()

optimizerG = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizerD = optim.Adam(disc.parameters(), lr=0.0001, betas=(0.5, 0.999))

import torch
import torchvision
import os

os.makedirs("outputs", exist_ok=True)

num_epochs = 100
noise_dim = 64

for epoch in range(num_epochs):
    for i, real_images in enumerate(dataLoader):
        real_images = real_images.to(device)

        # Label smoothing
        outputs = disc(real_images)
        real_labels = torch.full_like(outputs, 0.9)
        d_loss_real = criterion(outputs, real_labels)

        noise = torch.randn(real_images.size(0), noise_dim, 1, 1, device=device)
        fake_images = gen(noise)

        outputs = disc(fake_images.detach())
        fake_labels = torch.zeros_like(outputs)
        d_loss_fake = criterion(outputs, fake_labels)

        d_loss = d_loss_real + d_loss_fake
        optimizerD.zero_grad()
        d_loss.backward()
        optimizerD.step()

        # Train G
        noise = torch.randn(real_images.size(0), noise_dim, 1, 1, device=device)
        fake_images = gen(noise)
        outputs = disc(fake_images)
        g_loss = criterion(outputs, real_labels)

        optimizerG.zero_grad()
        g_loss.backward()
        optimizerG.step()

        # Save snapshots at milestone epochs
        if (epoch + 1) in [20, 50, 100] and i == 0:
            torchvision.utils.save_image(
                fake_images[:64], f"outputs/epoch_{epoch+1:03d}.png", normalize=True
            )

        if (i+1) % 100 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataLoader)}], "
                  f"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}")

import matplotlib.pyplot as plt
import torchvision.utils as vutils
from PIL import Image
import os

def show_image_grid(image_path, title, nrow=8):
    img = Image.open(image_path)
    img_tensor = vutils.make_grid(torchvision.transforms.ToTensor()(img), nrow=nrow, normalize=True)
    plt.figure(figsize=(6,6))
    plt.axis("off")
    plt.title(title)
    plt.imshow(img_tensor.permute(1, 2, 0).numpy())
    plt.show()

# Comparison between baseline and enhanced models
epoch_paths = {
    "Baseline (Epoch 20)": "outputs/epoch_020.png",
    "Improved (Epoch 50)": "outputs/epoch_050.png",
    "Improved (Epoch 100)": "outputs/epoch_100.png"
}

for title, path in epoch_paths.items():
    if os.path.exists(path):
        show_image_grid(path, title)
    else:
        print(f"‚ùå Missing: {path}")

